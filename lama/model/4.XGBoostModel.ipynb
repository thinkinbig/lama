{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with XGBoost\n",
    "\n",
    "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from data import DATA_DIR\n",
    "from lama.util.decorators import enable_logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "OUT_DIR = os.path.join(DATA_DIR, \"pre\")\n",
    "RESULT_DIR = os.path.join(DATA_DIR, \"result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(OUT_DIR, \"train_groupby.csv\"))\n",
    "test = pd.read_csv(os.path.join(OUT_DIR, \"test_groupby.csv\"))\n",
    "\n",
    "features = train.columns.tolist()\n",
    "features.remove(\"target\")\n",
    "features.remove(\"card_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see <a href=https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python />\n",
    "param_init = {\n",
    "    'objective': 'reg:linear',\n",
    "    # analog to learning rate in gbm\n",
    "    'eta': 0.3,\n",
    "    # similar to min_child_leaf in GBM but not exactly.\n",
    "    'min_child_weight': 0.8,\n",
    "    # L2 regularization\n",
    "    'lambda': 0.2,\n",
    "    # L1 regularization\n",
    "    'alpha': 0.5,\n",
    "    'eval_metric': 'rmse',\n",
    "    'seed': 2022,\n",
    "    'early_stopping_rounds': 30,\n",
    "    'num_boost_round': 1000,\n",
    "    'verbose_eval': 50,\n",
    "}\n",
    "\n",
    "\n",
    "def dump(model, filename):\n",
    "    with open(os.path.join(RESULT_DIR, filename), 'w') as file:\n",
    "        model.dump_model(file, dump_format=\"json\")\n",
    "\n",
    "\n",
    "@enable_logging(\"perform_xgb_kfold.log\")\n",
    "def perform_xgb_kfold(train, test, features, target, params, n_split=5, random_state=22):\n",
    "    kf = KFold(n_splits=n_split, random_state=random_state, shuffle=True)\n",
    "    prediction_test = 0\n",
    "    eval_results= {}\n",
    "    predictions = np.zeros(train.shape[0])\n",
    "    for i, (train_index, validation_index) in enumerate(kf.split(train[features])):\n",
    "        train_xgb = xgb.DMatrix(train[features].loc[train_index].values,\n",
    "                                train[target].loc[train_index].values)\n",
    "        validation_xgb = xgb.DMatrix(train[features].loc[validation_index].values,\n",
    "                                     train[target].loc[validation_index].values)\n",
    "\n",
    "        bst: xgb.Booster  = xgb.train(params, train_xgb,\n",
    "                        num_boost_round=params['num_boost_round'],\n",
    "                        early_stopping_rounds=params['early_stopping_rounds'],\n",
    "                        evals_result=eval_results,\n",
    "                        evals=[(train_xgb, 'train'), (validation_xgb, 'eval')],\n",
    "                        verbose_eval=params['verbose_eval'])\n",
    "        logger.debug(f'evaluate results in round {i}: {eval_results}')\n",
    "\n",
    "        prediction_test += bst.predict(xgb.DMatrix(test[features].values))\n",
    "        validation_pre = bst.predict(validation_xgb)\n",
    "        score = np.sqrt(mean_squared_error(train[target].loc[validation_index].values, validation_pre))\n",
    "        logger.debug(f'CV Score in {i} round: {score}')\n",
    "\n",
    "        predictions[validation_index] = validation_pre\n",
    "        dump(bst, f'XGBoost_{i}.json')\n",
    "    return prediction_test, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_split = 5\n",
    "prediction_test, predictions = perform_xgb_kfold(train, test, features, 'target', param_init, n_split=n_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['target'] = prediction_test / n_split\n",
    "test[['card_id', 'target']].to_csv(os.path.join(RESULT_DIR, \"submission_xgboost_kfold.csv\"), index=False)\n",
    "\n",
    "\n",
    "prediction_csv = pd.read_csv(os.path.join(OUT_DIR, \"prediction_train.csv\"))\n",
    "prediction_csv['target_4'] = predictions\n",
    "# write also actual target\n",
    "prediction_csv['target'] = train['target']\n",
    "prediction_csv.to_csv(os.path.join(OUT_DIR, \"prediction_train.csv\"), index=False)\n",
    "\n",
    "prediction_csv = pd.read_csv(os.path.join(OUT_DIR, \"prediction_test.csv\"))\n",
    "prediction_csv['target_4'] = prediction_test\n",
    "prediction_csv.to_csv(os.path.join(OUT_DIR, \"prediction_test.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_train_validation(train, validation, target):\n",
    "    plt.figure(figsize=(14,4))\n",
    "    train_target = train[target]\n",
    "    plt.plot(train_target, label='train')\n",
    "    plt.plot(validation, label='validation')\n",
    "    plt.xlabel(\"numbers\")\n",
    "    plt.ylabel(target)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "plot_train_validation(train, predictions, 'target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking with previous model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implements a simple stacking model that takes only one layer.\n",
    "Actually we know that there are many available frameworks like `autogluon` in automl.\n",
    " But this will make less fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step one read datas\n",
    "prediction_test = pd.read_csv(os.path.join(OUT_DIR, \"prediction_test.csv\"))\n",
    "prediction_train = pd.read_csv(os.path.join(OUT_DIR, \"prediction_train.csv\"))\n",
    "\n",
    "def get_prediction_as_stack(prediction):\n",
    "    return prediction[['target_1', 'target_2', 'target_3', 'target_4']].values\n",
    "\n",
    "train_hstack = get_prediction_as_stack(prediction_train)\n",
    "test_hstack = get_prediction_as_stack(prediction_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step two write our own kfold-stacking\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "n_splits, n_repeats, random_state = 5, 2, 2022\n",
    "\n",
    "@enable_logging(\"perform_stacking_kfold_with_gradient.log\")\n",
    "def perform_stacking_kfold_with_gradient(train_hstack, test_hstack, y, n_splits=n_splits, n_repeats=n_repeats):\n",
    "    rf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=2022)\n",
    "    prediction_test = np.zeros(test_hstack.shape[0])\n",
    "    # n_repeats * n_spilts in total\n",
    "    for i, (train_index, validation_index) in enumerate(rf.split(train_hstack)):\n",
    "        logger.debug(f\"Fold in {i}\")\n",
    "        train_x, train_y = train_hstack[train_index], y[train_index]\n",
    "        val_x, val_y = train_hstack[validation_index], y[validation_index]\n",
    "        clf = MLPRegressor(activation='relu',\n",
    "                           learning_rate='adaptive',\n",
    "                           early_stopping=False)\n",
    "        clf.fit(train_x, train_y)\n",
    "        score = clf.score(val_x, val_y)\n",
    "        logger.debug(f\"Score in Fold {i} is {score}\")\n",
    "        prediction_test += clf.predict(test_hstack)\n",
    "    prediction_test = prediction_test / (n_splits * n_repeats)\n",
    "    return prediction_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test = perform_stacking_kfold_with_gradient(train_hstack, test_hstack, prediction_train['target'])\n",
    "\n",
    "test['target'] = prediction_test\n",
    "test[['card_id', 'target']].to_csv(os.path.join(RESULT_DIR, \"submission_stacking_kfold.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ccbb4ca7fe902b23c4d3bf398cc69bd54ad114cdf2cf797081a3177c4f0863aa"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('lama_gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
